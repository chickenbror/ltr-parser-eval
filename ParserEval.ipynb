{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_path = 'ud/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_file(filename, outfile):\n",
    "    with open(filename, \"r\", encoding='utf8') as f, open(outfile, 'w', encoding='utf8') as o:\n",
    "        i=0\n",
    "        for line in f:\n",
    "            if line[:9] == '# text = ': \n",
    "                text = line[9:]\n",
    "                i+=1\n",
    "                o.write(text)\n",
    "    print(f'Wrote {i} sentences to file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5243 sentences to file.\n",
      "Wrote 4054 sentences to file.\n",
      "Wrote 1000 sentences to file.\n"
     ]
    }
   ],
   "source": [
    "make_text_file('conllu-files/en_formal.conllu', 'en_formal.txt')\n",
    "make_text_file('conllu-files/en_literature.conllu', 'en_literature.txt')\n",
    "make_text_file('conllu-files/en_news.conllu', 'en_news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5243 sentences to file.\n",
      "Wrote 4054 sentences to file.\n",
      "Wrote 1000 sentences to file.\n"
     ]
    }
   ],
   "source": [
    "make_text_file('conllu-files/sv_formal.conllu', 'sv_formal.txt')\n",
    "make_text_file('conllu-files/sv_literature.conllu', 'sv_literature.txt')\n",
    "make_text_file('conllu-files/sv_news.conllu', 'sv_news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(filename):\n",
    "    '''Returns a dict of sentence_text:{'upos':[upos_tags...], 'xpos':[xpos_tags}'''\n",
    "    \n",
    "    with open(filename, \"r\", encoding='utf8') as f:\n",
    "        \n",
    "        sentences = {}\n",
    "        tokens = []\n",
    "        upos_tags = []\n",
    "        xpos_tags = []\n",
    "        deprels = []\n",
    "        \n",
    "        \n",
    "        for line in f:\n",
    "            if line[0] == '#': #skip '# newdoc id = n01001', '# sent_id = n01001011'\n",
    "                continue\n",
    "            columns = line.split() # 10 cols\n",
    "            if columns == []: # When reading a blank line => finish reading one sentence\n",
    "                sentence_text = tuple(tokens)\n",
    "                sentences[sentence_text] = {}\n",
    "                sentences[sentence_text]['upos'] = upos_tags\n",
    "                sentences[sentence_text]['xpos'] = xpos_tags\n",
    "                sentences[sentence_text]['deprel'] = deprels\n",
    "                tokens, upos_tags, xpos_tags, deprels = [], [], [], [] # Reset the pos lists\n",
    "                continue\n",
    "            tokens.append(columns[1])\n",
    "            upos_tags.append(columns[3])\n",
    "            xpos_tags.append(columns[4])\n",
    "            deprels.append((columns[6], columns[7])) # head id, deprel tag\n",
    "            \n",
    "\n",
    "            \n",
    "    print(f'Corpus contains {len(sentences)} sentences.')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus contains 5227 sentences.\n",
      "Corpus contains 4046 sentences.\n",
      "Corpus contains 1000 sentences.\n"
     ]
    }
   ],
   "source": [
    "eng_formal = read_conllu('conllu-files/en_formal.conllu')\n",
    "eng_literature = read_conllu('conllu-files/en_literature.conllu')\n",
    "eng_news = read_conllu('conllu-files/en_news.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_sparv_xml(filename):\n",
    "    '''Reads the XML file from Sparv annotation and return the texts and \n",
    "    the POS and MSD tags'''\n",
    "    with open(filename, \"r\", encoding='utf8') as f:\n",
    "        \n",
    "        sentences = {}\n",
    "        \n",
    "        for line in f:\n",
    "            if line[:3]=='<se':\n",
    "                tokens = []\n",
    "                pos_tags = []\n",
    "                msd_tags = []\n",
    "                deprels = [] \n",
    "                \n",
    "            if line[:3]=='<w ':\n",
    "                token = re.findall('(?<=>)[^\\s]+(?=</w>)', line)[0]\n",
    "                pos = re.findall('(?<=pos=\")[^\\s]+(?=\"\\sm)', line)[0]\n",
    "                msd = re.findall('(?<=msd=\")[^\\s]+(?=\"\\s)', line)[0]\n",
    "                try:\n",
    "                    head_id = re.findall('(?<=dephead=\")[^\\s]*(?=\"\\s)', line)[0]\n",
    "                    deprel_tag = re.findall('(?<=deprel=\")[^\\s]+(?=\">)', line)[0]\n",
    "                    \n",
    "                except IndexError:\n",
    "                    head_id, deprel_tag = '_', '_'\n",
    "                \n",
    "                if msd[0]=='F' and msd[1:].islower():\n",
    "                    msd = token # change tag to the punctuation itself\n",
    "                if pos=='PROPN' and len(token.split('_'))>1: #split multiword PropN \n",
    "                    propn_tokens = token.split('_')\n",
    "                    tokens.extend(propn_tokens)\n",
    "                    pos_tags.extend([pos]*len(propn_tokens))\n",
    "                    msd_tags.extend([msd]*len(propn_tokens))\n",
    "                    deprels.extend([(head_id, deprel_tag)]*len(propn_tokens))\n",
    "                    continue\n",
    "                \n",
    "                tokens.append(token)\n",
    "                pos_tags.append(pos)\n",
    "                msd_tags.append(msd)\n",
    "                deprels.append((head_id, deprel_tag))\n",
    "            \n",
    "            if line[:3]=='</s':\n",
    "                sentence_text = tuple(tokens)\n",
    "                sentences[sentence_text] = {}\n",
    "                sentences[sentence_text]['pos'] = pos_tags\n",
    "                sentences[sentence_text]['msd'] = msd_tags\n",
    "                sentences[sentence_text]['deprel'] = deprels\n",
    "                \n",
    "    print(f'Parser output contains {len(sentences)} sentences.')       \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser output contains 986 sentences.\n"
     ]
    }
   ],
   "source": [
    "sparv_output = read_sparv_xml('ud-eng-pud-parsed.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser output contains 22 sentences.\n"
     ]
    }
   ],
   "source": [
    "sparv_output_sv = read_sparv_xml('korpus.xml')\n",
    "\n",
    "# TODDO the Sparv parser outputs only 986 sentences... find a way to force it to output 1000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(list1, list2):\n",
    "    # if num of tokens is the same\n",
    "    if len(list1)==len(list2): \n",
    "        matched_count = sum(1 if list1[i]==list2[i] else 0 for i in range(len(list1)))\n",
    "        \n",
    "    else:\n",
    "        matched_count = 0\n",
    "    \n",
    "    return matched_count / len(list1) # matched / num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "674"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlaps = [x for x in sparv_output.keys() if x in ud_eng_pud.keys()]\n",
    "len(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'VERB']\n",
      "['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "sparv_pos_set=set()\n",
    "for text, anno in sparv_output.items():\n",
    "    pos = anno['pos']\n",
    "    sparv_pos_set.update(pos)\n",
    "print(sorted(sparv_pos_set))\n",
    "\n",
    "corpus_pos_set=set()\n",
    "for text, anno in ud_eng_pud.items():\n",
    "    pos = anno['upos']\n",
    "    corpus_pos_set.update(pos)\n",
    "print(sorted(corpus_pos_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8296837289373824"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_accumulated = 0\n",
    "eval_count=0\n",
    "for key in overlaps:\n",
    "    try:\n",
    "        parsed_pos = sparv_output[key]['pos']\n",
    "        gold_pos = ud_eng_pud[key]['upos']\n",
    "        score = match(parsed_pos, gold_pos)\n",
    "        score_accumulated+=score\n",
    "        eval_count+=1\n",
    "    except KeyError:\n",
    "        pass\n",
    "score_accumulated/eval_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_to_swe_equivalent(eng_corpus, swe_corpus, out_fn):\n",
    "    \n",
    "    with open(out_fn, \"w\", encoding='utf8') as o:\n",
    "    \n",
    "        for eng_corpora, swe_corpora in list(zip(eng_corpus,swe_corpus)):\n",
    "            with open(eng_corpora, \"r\", encoding='utf8') as en, open(swe_corpora, \"r\", encoding='utf8') as sv:\n",
    "\n",
    "                swe_sent_ids = []\n",
    "                for line in en:\n",
    "                    if line[:14] == '# sent_id = en': # sent_id = en_lines-ud-dev-doc2-3296\n",
    "                        swe_sent_id = 'sv' + line[14:].rstrip('\\n')\n",
    "                        swe_sent_ids.append(swe_sent_id)\n",
    "\n",
    "\n",
    "                sv_lines = [l.rstrip('\\n') for l in sv]\n",
    "                for line in sv_lines:\n",
    "                    if line[:12] == '# sent_id = ':\n",
    "                        send_id = line[12:]\n",
    "                        if send_id in swe_sent_ids:\n",
    "                            i = sv_lines.index(line)\n",
    "                            while sv_lines[i]!='':\n",
    "                                o.write(sv_lines[i]+'\\n') # write send_id ~ last token\n",
    "                                i+=1\n",
    "                            o.write('\\n')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eng_to_swe_equivalent([f'domains/literature/en_lines-ud-{x}.conllu' for x in ['train', 'dev', 'test']], \n",
    "                     [f'UD/UD_Swedish-LinES/sv_lines-ud-{x}.conllu' for x in ['train', 'dev', 'test']],\n",
    "                     'sv_literature.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus contains 4048 sentences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4048"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(read_conllu('sv_literature.conllu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_corpus(corpus, out_fn):\n",
    "    with open(out_fn, \"w\", encoding='utf8') as o:\n",
    "        for corpora in corpus:\n",
    "            with open(corpora, \"r\", encoding='utf8') as f:\n",
    "                for line in f:\n",
    "                    o.write(line)\n",
    "merge_corpus([f'domains/literature/en_lines-ud-{x}.conllu' for x in ['train', 'dev', 'test']],\n",
    "            'en_literature.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus contains 4046 sentences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4046"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(read_conllu('en_literature.conllu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formal_corpus(lines_fn, lit_fn, out_fn):\n",
    "    # LinEs - Literature = formal\n",
    "    with open(out_fn, \"w\", encoding='utf8') as o:\n",
    "\n",
    "        with open(lines_fn, \"r\", encoding='utf8') as lines, open(lit_fn, \"r\", encoding='utf8') as lit:\n",
    "            \n",
    "            lit_ids = []\n",
    "            for line in lit:\n",
    "                if line[:14] == '# sent_id = en': # sent_id = en_lines-ud-dev-doc2-3296\n",
    "                    lit_id = 'sv' + line[12:].rstrip('\\n')\n",
    "                    lit_ids.append(lit_id)\n",
    "                    \n",
    "            lines_l = [l.rstrip('\\n') for l in lines]\n",
    "            for line in lines_l:\n",
    "                if line[:12] == '# sent_id = ':\n",
    "                    send_id = line[12:]\n",
    "                    if send_id not in lit_ids:\n",
    "                        i = lines_l.index(line)\n",
    "                        while lines_l[i]!='':\n",
    "                            o.write(lines_l[i]+'\\n') # write send_id ~ last token\n",
    "                            i+=1\n",
    "                        o.write('\\n')\n",
    "                            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
