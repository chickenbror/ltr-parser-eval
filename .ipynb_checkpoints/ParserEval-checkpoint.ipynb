{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_path = 'ud/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_file(filename, outfile):\n",
    "    with open(filename, \"r\", encoding='utf8') as f, open(outfile, 'w', encoding='utf8') as o:\n",
    "        i=0\n",
    "        for line in f:\n",
    "            if line[:9] == '# text = ': \n",
    "                text = line[9:]\n",
    "                i+=1\n",
    "                o.write(text)\n",
    "    print(f'Wrote {i} sentences to file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1000 sentences to file.\n"
     ]
    }
   ],
   "source": [
    "make_text_file(ud_path+'UD_English-PUD/en_pud-ud-test.conllu', 'ud-eng-pud-sents.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(filename):\n",
    "    '''Returns a dict of sentence_text:{'upos':[upos_tags...], 'xpos':[xpos_tags}'''\n",
    "    \n",
    "    with open(filename, \"r\", encoding='utf8') as f:\n",
    "        \n",
    "        sentences = {}\n",
    "        tokens = []\n",
    "        upos_tags = []\n",
    "        xpos_tags = []\n",
    "        \n",
    "        \n",
    "        for line in f:\n",
    "            if line[0] == '#': #skip '# newdoc id = n01001', '# sent_id = n01001011'\n",
    "                continue\n",
    "            columns = line.split() # 10 cols\n",
    "            if columns == []: # When reading a blank line => finish reading one sentence\n",
    "                sentence_text = ' '.join(tokens)\n",
    "                sentences[sentence_text] = {}\n",
    "                sentences[sentence_text]['upos'] = upos_tags\n",
    "                sentences[sentence_text]['xpos'] = xpos_tags\n",
    "                tokens, upos_tags, xpos_tags = [], [], [] # Reset the pos lists\n",
    "                continue\n",
    "            tokens.append(columns[1])\n",
    "            upos_tags.append(columns[3])\n",
    "            xpos_tags.append(columns[4])\n",
    "            \n",
    "\n",
    "            \n",
    "    print(f'Corpus contains {len(sentences)} sentences.')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus contains 1000 sentences.\n"
     ]
    }
   ],
   "source": [
    "ud_eng_pud = read_conllu(ud_path+'UD_English-PUD/en_pud-ud-test.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = '<w pos=\"PM\" msd=\"PM.NOM\" lemma=\"|Ikea|\" lex=\"|Ikea..pm.1|\" sense=\"|IKEA..1:-1.000|Ikea..1:-1.000|\" complemgram=\"|\" compwf=\"|\" ref=\"01\" dephead=\"\" deprel=\"SS\">Ikea</w>'\n",
    "re.findall('(?<=dephead=\").*(?=\"\\s)', t)[0]\n",
    "# re.findall('(?<=deprel=\").+(?=\">)', t)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_sparv_xml(filename):\n",
    "    '''Reads the XML file from Sparv annotation and return the texts and \n",
    "    the POS and MSD tags'''\n",
    "    with open(filename, \"r\", encoding='utf8') as f:\n",
    "        \n",
    "        sentences = {}\n",
    "        \n",
    "        for line in f:\n",
    "            if line[:3]=='<se':\n",
    "                tokens = []\n",
    "                pos_tags = []\n",
    "                msd_tags = []\n",
    "                deprels = [] \n",
    "                \n",
    "            if line[:3]=='<w ':\n",
    "                token = re.findall('(?<=>).+(?=</w>)', line)[0]\n",
    "                pos = re.findall('(?<=pos=\").+(?=\"\\sm)', line)[0]\n",
    "                msd = re.findall('(?<=msd=\").+(?=\"\\s)', line)[0]\n",
    "                try:\n",
    "                    head_id = re.findall('(?<=dephead=\").*(?=\"\\s)', line)[0]\n",
    "                    deprel_tag = re.findall('(?<=deprel=\").+(?=\">)', line)[0]\n",
    "                    \n",
    "                except IndexError:\n",
    "                    head_id, deprel_tag = '_', '_'\n",
    "                \n",
    "                if msd[0]=='F' and msd[1:].islower():\n",
    "                    msd = token # change tag to the punctuation itself\n",
    "                if pos=='PROPN' and len(token.split('_'))>1: #split multiword PropN \n",
    "                    propn_tokens = token.split('_')\n",
    "                    tokens.extend(propn_tokens)\n",
    "                    pos_tags.extend([pos]*len(propn_tokens))\n",
    "                    msd_tags.extend([msd]*len(propn_tokens))\n",
    "                    deprels.extend([(head_id, deprel_tag)]*len(propn_tokens))\n",
    "                    continue\n",
    "                \n",
    "                tokens.append(token)\n",
    "                pos_tags.append(pos)\n",
    "                msd_tags.append(msd)\n",
    "                deprels.append((head_id, deprel_tag))\n",
    "            \n",
    "            if line[:3]=='</s':\n",
    "                sentence_text = \" \".join(tokens)\n",
    "                sentences[sentence_text] = {}\n",
    "                sentences[sentence_text]['pos'] = pos_tags\n",
    "                sentences[sentence_text]['msd'] = msd_tags\n",
    "                \n",
    "    print(f'Parser output contains {len(sentences)} sentences.')       \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser output contains 986 sentences.\n"
     ]
    }
   ],
   "source": [
    "sparv_output = read_sparv_xml('ud-eng-pud-parsed.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser output contains 22 sentences.\n"
     ]
    }
   ],
   "source": [
    "sparv_output_sv = read_sparv_xml('korpus.xml')\n",
    "\n",
    "# TODDO the Sparv parser outputs only 986 sentences... find a way to force it to output 1000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def match(list1, list2):\n",
    "    if len(list1)==len(list2):\n",
    "        matched_count = sum(1 if list1[i]==list2[i] else 0 for i in range(len(list1)))\n",
    "        return matched_count\n",
    "    else:\n",
    "        return 0\n",
    "match(ud_eng_pud[key1]['xpos'], sparv_output[key1]['msd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "674"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlaps = [x for x in sparv_output.keys() if x in ud_eng_pud.keys()]\n",
    "len(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
